{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Below is word2vec+lemmatization**\n"
      ],
      "metadata": {
        "id": "XW4oTAeI-WkF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMm8yBfBYbY5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import random\n",
        "import pickle\n",
        "import os \n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import datetime\n",
        "\n",
        "# Create a log directory  \n",
        "import os\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"-model1\"\n",
        "os.makedirs(log_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "rrkKqdVnPlVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Read the file\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/nietzsche.txt', 'r') as file:\n",
        "    text = file.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LINBOxWwYqHG",
        "outputId": "25278f4b-8de3-4cf0-ef83-167549364ffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = text.split()\n",
        "unique_words = set(words)\n",
        "num_unique_words = len(unique_words)\n",
        "\n",
        "print(\"Number of unique words:\", num_unique_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IptFR3rO9A3j",
        "outputId": "0cad5603-58b3-4d93-ec53-1d6a9098bae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words: 18809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roOLBAXOaJSr",
        "outputId": "90250e4a-1021-4d5b-a822-f8073b40cf93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "def preprocessing(text, lemmatize=True, stemming=False, word2vec=True, return_type='word_sequences'):\n",
        "    # Step 1: Make lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Step 2: Remove punctuations\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Step 3: Remove numbers\n",
        "    wordonly = [word for word in text.split() if not word.isdigit()]\n",
        "    text = ' '.join(wordonly)\n",
        "\n",
        "    # Step 4: Lemmatize the words\n",
        "    if lemmatize:\n",
        "        lemmatization = WordNetLemmatizer()\n",
        "        text = [lemmatization.lemmatize(word) for word in text.split()]\n",
        "\n",
        "    # Step 5: Stem the words\n",
        "    if stemming:\n",
        "        stemmer = PorterStemmer()\n",
        "        text = text.split()\n",
        "        text = [stemmer.stem(word) for word in text]\n",
        "\n",
        "    text = ' '.join(text)\n",
        "\n",
        "    if word2vec:\n",
        "        # Initialize Word2Vec model\n",
        "        model = Word2Vec([text.split()], min_count=1, vector_size=100)\n",
        "        word_sequences = np.array([model.wv[word] for word in text.split()])\n",
        "        vocab_size = len(model.wv)\n",
        "        vec_to_index = {tuple(model.wv[word]): i for i, word in enumerate(model.wv.index_to_key)}\n",
        "\n",
        "\n",
        "    if return_type == 'word_sequences':\n",
        "        return word_sequences\n",
        "    elif return_type == 'vocab_size':\n",
        "        return vocab_size\n",
        "    elif return_type == 'vec_to_index':\n",
        "        return vec_to_index\n",
        "    else:\n",
        "        raise ValueError('Invalid return type')\n"
      ],
      "metadata": {
        "id": "t_3fAQdCaNlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec_to_index=preprocessing(text, lemmatize=True, stemming=False, word2vec=True, return_type='vec_to_index')\n",
        "\n",
        "vec_to_index_stemming=preprocessing(text, lemmatize=False, stemming=True, word2vec=True, return_type='vec_to_index')\n",
        "\n"
      ],
      "metadata": {
        "id": "F0R1cO87ph3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_sequences=preprocessing(text, lemmatize=True, stemming=False, word2vec=True)"
      ],
      "metadata": {
        "id": "83N48rfsMCHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=preprocessing(text, lemmatize=True, stemming=False, word2vec=True, return_type='vocab_size')\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SG1PcJNO375",
        "outputId": "08e07e2b-117c-47c2-9dae-d7c2937cbdf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split word vectors into input sequences and corresponding output labels\n",
        "prev_words = []\n",
        "next_words = []\n",
        "\n",
        "for i in range(4, len(word_sequences)-4):\n",
        "    prev_words.append(word_sequences[i-4:i])\n",
        "    next_words.append(word_sequences[i])\n",
        "\n",
        "# Convert the lists to numpy arrays for use in the LSTM model\n",
        "X_word2vec = np.array(prev_words)\n",
        "y_word2vec = np.array(next_words)\n"
      ],
      "metadata": {
        "id": "xRQn20OmLkzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_word2vec.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-I9ur40MhQ4",
        "outputId": "a7b55b1c-db2c-4be8-f2a8-f28d9977a863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(98607, 4, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_word2vec.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plW_e8t8MkaT",
        "outputId": "0ae1004e-e4a0-4554-c6fb-9d555b7407c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(98607, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_word2vec = X_word2vec.reshape(98611, 4, 100)\n",
        "# y_word2vec = y_word2vec.reshape(98611, 100)\n"
      ],
      "metadata": {
        "id": "SyCTzl7HZvcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_word2vec_indices = np.array([vec_to_index[tuple(vec)] for vec in y_word2vec])\n"
      ],
      "metadata": {
        "id": "q32NxnYMpQl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.keras.backend.clear_session()\n"
      ],
      "metadata": {
        "id": "RH41Aijaudg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.layers import Input\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "filepath='/content/drive/MyDrive/Colab Notebooks/best_model_word2vec_lemmatize.h5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "num_layers = 2\n",
        "lstm_units_0 = 128\n",
        "recurrent_dropout_0 = 0.2\n",
        "lstm_units_1 = 128\n",
        "recurrent_dropout_1 = 0.1\n",
        "dropout_rate = 0.1\n",
        "learning_rate = 0.005977728042983696\n",
        "\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Input(shape=(4, 100)))\n",
        "model_lstm.add(LSTM(lstm_units_0, return_sequences=True, recurrent_dropout=recurrent_dropout_0))\n",
        "model_lstm.add(LSTM(lstm_units_1, recurrent_dropout=recurrent_dropout_1))\n",
        "model_lstm.add(Dropout(dropout_rate))\n",
        "model_lstm.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "# Train the model\n",
        "model_lstm.fit(X_word2vec, y_word2vec_indices, epochs=5, batch_size=128, validation_split=0.2, callbacks=[tensorboard_callback,checkpoint])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJ7dHHCU6fol",
        "outputId": "4cc2c294-af53-46bf-8e80-79ece2e4b88a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "616/617 [============================>.] - ETA: 0s - loss: 6.8923 - accuracy: 0.0699\n",
            "Epoch 1: val_accuracy improved from -inf to 0.09320, saving model to /content/drive/MyDrive/Colab Notebooks/best_model_word2vec_lemmatize.h5\n",
            "617/617 [==============================] - 41s 50ms/step - loss: 6.8924 - accuracy: 0.0699 - val_loss: 6.6255 - val_accuracy: 0.0932\n",
            "Epoch 2/5\n",
            "617/617 [==============================] - ETA: 0s - loss: 6.4979 - accuracy: 0.0856\n",
            "Epoch 2: val_accuracy improved from 0.09320 to 0.09375, saving model to /content/drive/MyDrive/Colab Notebooks/best_model_word2vec_lemmatize.h5\n",
            "617/617 [==============================] - 34s 55ms/step - loss: 6.4979 - accuracy: 0.0856 - val_loss: 6.6610 - val_accuracy: 0.0938\n",
            "Epoch 3/5\n",
            "616/617 [============================>.] - ETA: 0s - loss: 6.3693 - accuracy: 0.0909\n",
            "Epoch 3: val_accuracy improved from 0.09375 to 0.09563, saving model to /content/drive/MyDrive/Colab Notebooks/best_model_word2vec_lemmatize.h5\n",
            "617/617 [==============================] - 32s 52ms/step - loss: 6.3693 - accuracy: 0.0909 - val_loss: 6.6708 - val_accuracy: 0.0956\n",
            "Epoch 4/5\n",
            "616/617 [============================>.] - ETA: 0s - loss: 6.2590 - accuracy: 0.0928\n",
            "Epoch 4: val_accuracy did not improve from 0.09563\n",
            "617/617 [==============================] - 29s 46ms/step - loss: 6.2586 - accuracy: 0.0928 - val_loss: 6.7282 - val_accuracy: 0.0942\n",
            "Epoch 5/5\n",
            "617/617 [==============================] - ETA: 0s - loss: 6.1514 - accuracy: 0.0923\n",
            "Epoch 5: val_accuracy improved from 0.09563 to 0.09674, saving model to /content/drive/MyDrive/Colab Notebooks/best_model_word2vec_lemmatize.h5\n",
            "617/617 [==============================] - 30s 48ms/step - loss: 6.1514 - accuracy: 0.0923 - val_loss: 6.7694 - val_accuracy: 0.0967\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbee02df160>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorboard dev upload --logdir {log_dir} --name \"Word2vecprojectlemmatization\" --description \"Training results from https://colab.research.google.com\" --one_shot\n"
      ],
      "metadata": {
        "id": "IqWhAbm-69BS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3424687-c0c6-4bda-b12a-292d6bc91a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-14 00:45:08.895402: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "***** TensorBoard Uploader *****\n",
            "\n",
            "This will upload your TensorBoard logs to https://tensorboard.dev/ from\n",
            "the following directory:\n",
            "\n",
            "logs/fit/20230514-004112-model1\n",
            "\n",
            "This TensorBoard will be visible to everyone. Do not upload sensitive\n",
            "data.\n",
            "\n",
            "Your use of this service is subject to Google's Terms of Service\n",
            "<https://policies.google.com/terms> and Privacy Policy\n",
            "<https://policies.google.com/privacy>, and TensorBoard.dev's Terms of Service\n",
            "<https://tensorboard.dev/policy/terms/>.\n",
            "\n",
            "This notice will not be shown again while you are logged into the uploader.\n",
            "To log out, run `tensorboard dev auth revoke`.\n",
            "\n",
            "Continue? (yes/NO) yes\n",
            "\n",
            "To sign in with the TensorBoard uploader:\n",
            "\n",
            "1. On your computer or phone, visit:\n",
            "\n",
            "   https://www.google.com/device\n",
            "\n",
            "2. Sign in with your Google account, then enter:\n",
            "\n",
            "   PZZD-RXZX\n",
            "\n",
            "\n",
            "\n",
            "New experiment created. View your TensorBoard at: https://tensorboard.dev/experiment/Md4q009YTbWJi9j2M1RXmQ/\n",
            "\n",
            "\u001b[1m[2023-05-14T00:45:50]\u001b[0m Started scanning logdir.\n",
            "\u001b[1m[2023-05-14T00:45:51]\u001b[0m Total uploaded: 30 scalars, 40 tensors (28.7 kB), 1 binary objects (691.1 kB)\n",
            "\u001b[1m[2023-05-14T00:45:51]\u001b[0m Done scanning logdir.\n",
            "\n",
            "\n",
            "Done. View your TensorBoard at https://tensorboard.dev/experiment/Md4q009YTbWJi9j2M1RXmQ/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Below is word2vec+stemming**"
      ],
      "metadata": {
        "id": "0UoNM8cG7IBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir2 = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"-model2\"\n",
        "os.makedirs(log_dir2, exist_ok=True)"
      ],
      "metadata": {
        "id": "ym40cj70Vfd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec_to_index_stemming=preprocessing(text, lemmatize=False, stemming=True, word2vec=True, return_type='vec_to_index')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iKZ5jAWd7j2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_sequences_stemming=preprocessing(text, lemmatize=False, stemming=True, word2vec=True)\n",
        "word_sequences_stemming.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuMpJo197j2e",
        "outputId": "83eebe30-a695-4034-8945-43c2360be177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(98615, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size_stemming=preprocessing(text, lemmatize=False, stemming=True, word2vec=True, return_type='vocab_size')\n",
        "print(vocab_size_stemming)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a3991f1-9163-4111-c224-02cb89139185",
        "id": "z1gMunb97j2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split word vectors into input sequences and corresponding output labels\n",
        "prev_words_stemming = []\n",
        "next_words_stemming = []\n",
        "\n",
        "for i in range(4, len(word_sequences_stemming)-4):\n",
        "    prev_words_stemming.append(word_sequences_stemming[i-4:i])\n",
        "    next_words_stemming.append(word_sequences_stemming[i])\n",
        "\n",
        "# Convert the lists to numpy arrays for use in the LSTM model\n",
        "X_word2vec_stemming = np.array(prev_words_stemming)\n",
        "y_word2vec_stemming = np.array(next_words_stemming)"
      ],
      "metadata": {
        "id": "2FEUj2Ag8CjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_word2vec_stemming.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p1jRNAfBaFx",
        "outputId": "2b7cc519-466b-4eba-99e3-93aa8405d3ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(98607, 4, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_word2vec_indices_stemming = np.array([vec_to_index_stemming[tuple(vec)] for vec in y_word2vec_stemming])\n",
        "vocab_size_stemming=preprocessing(text, lemmatize=False, stemming=True, word2vec=True, return_type='vocab_size')\n"
      ],
      "metadata": {
        "id": "W3dI9I4O8LW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.keras.backend.clear_session()\n"
      ],
      "metadata": {
        "id": "chTho5b0ugB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.layers import Input\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "filepath='/content/drive/MyDrive/Colab Notebooks/best_model_word2vec_stemming.h5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "num_layers = 2\n",
        "lstm_units_0 = 128\n",
        "recurrent_dropout_0 = 0.2\n",
        "lstm_units_1 = 128\n",
        "recurrent_dropout_1 = 0.1\n",
        "dropout_rate = 0.1\n",
        "learning_rate = 0.005977728042983696\n",
        "\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Input(shape=(4, 100)))\n",
        "model_lstm.add(LSTM(lstm_units_0, return_sequences=True, recurrent_dropout=recurrent_dropout_0))\n",
        "model_lstm.add(LSTM(lstm_units_1, recurrent_dropout=recurrent_dropout_1))\n",
        "model_lstm.add(Dropout(dropout_rate))\n",
        "model_lstm.add(Dense(vocab_size_stemming, activation='softmax'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "tensorboard_callback2 = TensorBoard(log_dir=log_dir2, \n",
        "    histogram_freq=1,\n",
        "    embeddings_freq=1,\n",
        "    write_graph=True,\n",
        "    write_images=True,\n",
        "    update_freq='epoch', \n",
        "\n",
        ")\n",
        "# Train the model\n",
        "model_lstm.fit(X_word2vec_stemming, y_word2vec_indices_stemming, epochs=5, batch_size=128, validation_split=0.2, callbacks=[checkpoint, tensorboard_callback2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toj6Cg5A9ost",
        "outputId": "244dbf12-7597-43a3-e05c-c37fec0c2066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "616/617 [============================>.] - ETA: 0s - loss: 6.7059 - accuracy: 0.0687\n",
            "Epoch 1: val_accuracy improved from -inf to 0.09264, saving model to /content/drive/MyDrive/Colab Notebooks/best_model_word2vec_stemming.h5\n",
            "617/617 [==============================] - 37s 51ms/step - loss: 6.7061 - accuracy: 0.0687 - val_loss: 6.4429 - val_accuracy: 0.0926\n",
            "Epoch 2/5\n",
            "616/617 [============================>.] - ETA: 0s - loss: 6.3535 - accuracy: 0.0864\n",
            "Epoch 2: val_accuracy improved from 0.09264 to 0.09375, saving model to /content/drive/MyDrive/Colab Notebooks/best_model_word2vec_stemming.h5\n",
            "617/617 [==============================] - 31s 50ms/step - loss: 6.3535 - accuracy: 0.0864 - val_loss: 6.4363 - val_accuracy: 0.0938\n",
            "Epoch 3/5\n",
            "617/617 [==============================] - ETA: 0s - loss: 6.2559 - accuracy: 0.0901\n",
            "Epoch 3: val_accuracy did not improve from 0.09375\n",
            "617/617 [==============================] - 29s 47ms/step - loss: 6.2559 - accuracy: 0.0901 - val_loss: 6.4452 - val_accuracy: 0.0933\n",
            "Epoch 4/5\n",
            "617/617 [==============================] - ETA: 0s - loss: 6.1740 - accuracy: 0.0920\n",
            "Epoch 4: val_accuracy improved from 0.09375 to 0.09664, saving model to /content/drive/MyDrive/Colab Notebooks/best_model_word2vec_stemming.h5\n",
            "617/617 [==============================] - 29s 48ms/step - loss: 6.1740 - accuracy: 0.0920 - val_loss: 6.4795 - val_accuracy: 0.0966\n",
            "Epoch 5/5\n",
            "616/617 [============================>.] - ETA: 0s - loss: 6.0910 - accuracy: 0.0936\n",
            "Epoch 5: val_accuracy did not improve from 0.09664\n",
            "617/617 [==============================] - 29s 47ms/step - loss: 6.0908 - accuracy: 0.0936 - val_loss: 6.5189 - val_accuracy: 0.0958\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbee64b59f0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Show results in tensorboard**"
      ],
      "metadata": {
        "id": "ursO6ehT-VhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorboard dev upload --logdir {log_dir2} --name \"Word2vecprojectstemming\" --description \"Training results from https://colab.research.google.com\" --one_shot\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcwF6-MttP6_",
        "outputId": "3b56798d-d544-44bf-91b3-977d5181ed2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-14 00:48:40.402025: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "New experiment created. View your TensorBoard at: https://tensorboard.dev/experiment/L6uE3kLRSWed19ySYxqCoQ/\n",
            "\n",
            "\u001b[1m[2023-05-14T00:48:42]\u001b[0m Started scanning logdir.\n",
            "\u001b[1m[2023-05-14T00:48:44]\u001b[0m Total uploaded: 30 scalars, 40 tensors (28.7 kB), 1 binary objects (691.2 kB)\n",
            "\u001b[1m[2023-05-14T00:48:44]\u001b[0m Done scanning logdir.\n",
            "\n",
            "\n",
            "Done. View your TensorBoard at https://tensorboard.dev/experiment/L6uE3kLRSWed19ySYxqCoQ/\n"
          ]
        }
      ]
    }
  ]
}
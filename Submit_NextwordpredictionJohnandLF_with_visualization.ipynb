{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgmtJ4sp0Zpy"
      },
      "source": [
        "**Text source from project Gutenberg, Nietzsche text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciJ15ElfC2tr"
      },
      "source": [
        "**The link is here --> https://www.kaggle.com/datasets/pankrzysiu/nietzsche-texts**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhAZgj9lxEr1"
      },
      "source": [
        "**Results using keras tokenizer and lemmatization below**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMm8yBfBYbY5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import random\n",
        "import pickle\n",
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsCAF1724WiQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oPuCvU2pKOdE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LINBOxWwYqHG",
        "outputId": "f48f7c2e-80f9-404b-a73c-c2d20d5efa94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Read the file\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/Raw data/nietzsche.txt', 'r') as file:\n",
        "    text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTsB2b5A2D2l"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import datetime\n",
        "\n",
        "# Create a log directory\n",
        "import os\n",
        "log_dir1 = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"-tokenizermodel1\"\n",
        "os.makedirs(log_dir1, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roOLBAXOaJSr",
        "outputId": "e2d72848-3cdb-4e11-b841-c11382e93e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_3fAQdCaNlh"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "def preprocessing(text, lemmatize=True, stemming=False, tokenizer=None, return_type='word_sequences'):\n",
        "    # Step 1: Make lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Step 2: Remove punctuations\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Step 3: Remove numbers\n",
        "    wordonly = [word for word in text.split() if not word.isdigit()]\n",
        "    text = ' '.join(wordonly)\n",
        "\n",
        "    # Step 4: Lemmatize the words\n",
        "    if lemmatize:\n",
        "        lemmatization = WordNetLemmatizer()\n",
        "        text = [lemmatization.lemmatize(word) for word in text.split()]\n",
        "\n",
        "    # Step 5: Stem the words\n",
        "    if stemming:\n",
        "        stemmer = PorterStemmer()\n",
        "        text = text.split()\n",
        "        text = [stemmer.stem(word) for word in text]\n",
        "\n",
        "    text = ' '.join(text)\n",
        "\n",
        "    if tokenizer is None:\n",
        "        tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "\n",
        "    tokenizer.fit_on_texts([text])\n",
        "    word_sequences = tokenizer.texts_to_sequences([text])[0]\n",
        "    vocab_size = len(tokenizer.word_index)\n",
        "    unique_words = list(tokenizer.word_index.keys())\n",
        "\n",
        "\n",
        "    if return_type == 'word_sequences':\n",
        "        return word_sequences\n",
        "    elif return_type == 'vocab_size':\n",
        "        return vocab_size\n",
        "    elif return_type == 'unique_words':\n",
        "        return unique_words\n",
        "    elif return_type == 'tokenizer':\n",
        "        return tokenizer\n",
        "    elif return_type == 'text':\n",
        "        return text\n",
        "    else:\n",
        "        raise ValueError('Invalid return type')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')"
      ],
      "metadata": {
        "id": "CpCMqwQKlpvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw5-8udYjP6x",
        "outputId": "2e9b69d5-fcca-4514-f26d-2c5af4b77f3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10535\n"
          ]
        }
      ],
      "source": [
        "total_words=preprocessing(text, return_type='vocab_size', tokenizer=tokenizer)\n",
        "print(total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WTSnjBXck8c"
      },
      "outputs": [],
      "source": [
        "text_lemmatized_tokenizer = preprocessing(text, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min(text_lemmatized_tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKgLL6dyhzH5",
        "outputId": "712d3b90-23e2-44b0-9ba3-7e742eb7aec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_lemmatized_tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR5PD93PVnDG",
        "outputId": "d473cc31-6dac-4b70-d432-e0dcb9761657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98615"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0jWDKY7h3B3"
      },
      "outputs": [],
      "source": [
        "# Generate the sequences of four words and the next word in each sequence\n",
        "prev_words = []\n",
        "next_words = []\n",
        "\n",
        "for i in range(len(text_lemmatized_tokenizer)-4):\n",
        "    prev_words.append(text_lemmatized_tokenizer[i:i+4])\n",
        "    next_words.append(text_lemmatized_tokenizer[i+4])\n",
        "\n",
        "# Convert the lists to numpy arrays for use in the LSTM model\n",
        "X_tokenizer = np.array(prev_words)\n",
        "y_tokenizer = np.array(next_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yN4lAlR5iVyC"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_tokenizer, y_tokenizer, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-3UDoGaTx_I",
        "outputId": "122a914d-60b1-4ef7-87b0-c92f241ad56f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(78888, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcdKSXXgT03A",
        "outputId": "e40c9808-9313-498e-9837-9d5fc8433835"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(78888,)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfMj89aYnJ-2",
        "outputId": "4fdf8000-15a3-44c3-d958-49463562b25a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.3.5-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.27.1)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.3.5 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD32NA7BjrjA",
        "outputId": "66796f4f-aad6-4de9-c7bb-c509e3239d97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5 Complete [00h 04m 29s]\n",
            "val_accuracy: 0.12660345435142517\n",
            "\n",
            "Best val_accuracy So Far: 0.13071033358573914\n",
            "Total elapsed time: 00h 20m 00s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from kerastuner import HyperModel, RandomSearch\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from keras_tuner import RandomSearch\n",
        "from kerastuner.engine.hyperparameters import HyperParameters as hp\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "class LSTMHyperModel(HyperModel):\n",
        "    def __init__(self, input_length, total_words):\n",
        "        self.input_length = input_length\n",
        "        self.total_words = total_words\n",
        "\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(self.total_words, 100, input_length=self.input_length))\n",
        "        num_layers = hp.Int(\"num_layers\", 1, 3)\n",
        "        for i in range(num_layers):\n",
        "            return_sequences = True if i < num_layers - 1 else False\n",
        "            model.add(LSTM(hp.Int(f\"lstm_units_{i}\", 128, 256, step=128), return_sequences=return_sequences, recurrent_dropout=hp.Choice(f\"recurrent_dropout_{i}\", [0.1, 0.2, 0.3])))\n",
        "\n",
        "        model.add(Dropout(hp.Choice(\"dropout_rate\", [0.1, 0.2, 0.3])))\n",
        "        model.add(Dense(self.total_words, activation='softmax'))\n",
        "\n",
        "        model.compile(\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "            optimizer=tf.keras.optimizers.Adam(hp.Float(\"learning_rate\", 1e-4, 1e-2, sampling=\"log\")),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "\n",
        "input_length = 4\n",
        "hypermodel = LSTMHyperModel(input_length, total_words)\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    hypermodel,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=5,\n",
        "    seed=42,\n",
        "    executions_per_trial=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='accuracy', factor=0.8, patience=1, min_lr=0.0005, verbose=1)\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir1, histogram_freq=1)\n",
        "tuner.search(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_valid, y_valid),\n",
        "    callbacks=[\n",
        "        reduce_lr,\n",
        "        tensorboard_callback,\n",
        "        tf.keras.callbacks.TensorBoard(log_dir=log_dir1, update_freq='batch', profile_batch=0),\n",
        "    ],\n",
        ")\n",
        "\n",
        "\n",
        "best_model = tuner.get_best_models()[0]\n",
        "best_model.save('/content/drive/MyDrive/Colab Notebooks/best_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_NfwWsewMQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "971b6828-bbba-41e7-df99-f8067b13d081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters:\n",
            "num_layers: 2\n",
            "lstm_units_0: 128\n",
            "recurrent_dropout_0: 0.2\n",
            "lstm_units_1: 128\n",
            "recurrent_dropout_1: 0.1\n",
            "dropout_rate: 0.1\n",
            "learning_rate: 0.005977728042983696\n"
          ]
        }
      ],
      "source": [
        "best_hp = tuner.get_best_hyperparameters()[0]\n",
        "\n",
        "print(\"Best hyperparameters:\")\n",
        "print(f\"num_layers: {best_hp.get('num_layers')}\")\n",
        "for i in range(best_hp.get('num_layers')):\n",
        "    print(f\"lstm_units_{i}: {best_hp.get(f'lstm_units_{i}')}\")\n",
        "    print(f\"recurrent_dropout_{i}: {best_hp.get(f'recurrent_dropout_{i}')}\")\n",
        "print(f\"dropout_rate: {best_hp.get('dropout_rate')}\")\n",
        "print(f\"learning_rate: {best_hp.get('learning_rate')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_S8XJFo_4U61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e88b9107-5430-4b89-ae4c-8f333728e4b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-14 20:14:13.905538: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "***** TensorBoard Uploader *****\n",
            "\n",
            "This will upload your TensorBoard logs to https://tensorboard.dev/ from\n",
            "the following directory:\n",
            "\n",
            "logs/fit/20230514-195353-tokenizermodel1\n",
            "\n",
            "This TensorBoard will be visible to everyone. Do not upload sensitive\n",
            "data.\n",
            "\n",
            "Your use of this service is subject to Google's Terms of Service\n",
            "<https://policies.google.com/terms> and Privacy Policy\n",
            "<https://policies.google.com/privacy>, and TensorBoard.dev's Terms of Service\n",
            "<https://tensorboard.dev/policy/terms/>.\n",
            "\n",
            "This notice will not be shown again while you are logged into the uploader.\n",
            "To log out, run `tensorboard dev auth revoke`.\n",
            "\n",
            "Continue? (yes/NO) yes\n",
            "\n",
            "To sign in with the TensorBoard uploader:\n",
            "\n",
            "1. On your computer or phone, visit:\n",
            "\n",
            "   https://www.google.com/device\n",
            "\n",
            "2. Sign in with your Google account, then enter:\n",
            "\n",
            "   TCCC-VKVW\n",
            "\n",
            "\n",
            "\n",
            "New experiment created. View your TensorBoard at: https://tensorboard.dev/experiment/iLNwM4f2SL6iZqb8LAakzQ/\n",
            "\n",
            "\u001b[1m[2023-05-14T20:14:45]\u001b[0m Started scanning logdir.\n",
            "E0514 20:14:48.328963 139936522925888 uploader.py:1122] Attempted to re-upload existing blob.  Skipping.\n",
            "E0514 20:14:54.111122 139936522925888 uploader.py:1122] Attempted to re-upload existing blob.  Skipping.\n",
            "E0514 20:15:00.235914 139936522925888 uploader.py:1122] Attempted to re-upload existing blob.  Skipping.\n",
            "E0514 20:15:05.053854 139936522925888 uploader.py:1122] Attempted to re-upload existing blob.  Skipping.\n",
            "E0514 20:15:10.630577 139936522925888 uploader.py:1122] Attempted to re-upload existing blob.  Skipping.\n",
            "\u001b[1m[2023-05-14T20:15:14]\u001b[0m Total uploaded: 62050 scalars, 275 tensors (183.1 kB), 5 binary objects (4.1 MB)\n",
            "\u001b[90mTotal skipped: 5 binary objects (4.1 MB)\n",
            "\u001b[0m\u001b[1m[2023-05-14T20:15:14]\u001b[0m Done scanning logdir.\n",
            "\n",
            "\n",
            "Done. View your TensorBoard at https://tensorboard.dev/experiment/iLNwM4f2SL6iZqb8LAakzQ/\n"
          ]
        }
      ],
      "source": [
        "!tensorboard dev upload --logdir {log_dir1} --name \"tokenizerlemmatization\" --description \"Training results from https://colab.research.google.com\" --one_shot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUeefz-E3z2v"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.keras.backend.clear_session()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhslqndcrTJ1"
      },
      "source": [
        "**Results using keras tokenizer and stemming below**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGvr4PgyDnAS",
        "outputId": "8aaab6af-0416-4a6f-f812-e2023370652f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTOc0W7zDyfU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorboard.plugins import projector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjvZZNZylSdf"
      },
      "outputs": [],
      "source": [
        "text_stemming_tokenizer = preprocessing(text, lemmatize=False, stemming=True, tokenizer=tokenizer)  #This is the tokenized stemming text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "minimum_value = min(text_stemming_tokenizer)\n"
      ],
      "metadata": {
        "id": "0u4WD6qghGph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(minimum_value)"
      ],
      "metadata": {
        "id": "Dxe1oNn8iEom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea0951e6-eb53-4292-993b-7ba9c877f998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciAqzfQNy1VA",
        "outputId": "971044a0-d5ec-499f-b3f3-af527473518a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8060\n"
          ]
        }
      ],
      "source": [
        "vocab_size=preprocessing(text, lemmatize=False, stemming=True, return_type='vocab_size')\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNbVTGCQa8-S"
      },
      "outputs": [],
      "source": [
        "unique_words=preprocessing(text, lemmatize=False, stemming=True, return_type='unique_words')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(unique_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaytVVORhr04",
        "outputId": "39c7a823-5819-4cb2-ec8f-01c0f46f73c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8060"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_stemming_tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U-Pxrr6yiif",
        "outputId": "a94a5aae-7522-484f-e83b-e12f1165fb67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98615"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiQuI-fbzf94"
      },
      "outputs": [],
      "source": [
        "# Generate the sequences of four words and the next word in each sequence\n",
        "prev_words = []\n",
        "next_words = []\n",
        "\n",
        "for i in range(len(text_stemming_tokenizer)-4):\n",
        "    prev_words.append(text_stemming_tokenizer[i:i+4])\n",
        "    next_words.append(text_stemming_tokenizer[i+4])\n",
        "\n",
        "# Convert the lists to numpy arrays for use in the LSTM model\n",
        "X_tokenizer = np.array(prev_words)\n",
        "y_tokenizer = np.array(next_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n728MNkz1na"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_tokenizer, y_tokenizer, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Min in y_train:', min(y_train))\n",
        "print('Min in y_valid:', min(y_valid))\n"
      ],
      "metadata": {
        "id": "lA3EDZMKOAkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bac77854-f0f7-4203-f8a5-8a196aab154d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min in y_train: 1\n",
            "Min in y_valid: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgHUVQE_Ti88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "589c07e9-707f-4711-c7c3-973d6a434bf4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(78888, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNvPCy32Tk_r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6651fd5-e631-4eaf-b814-34815b60ade3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(78888,)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fs0NUt7J7HJx"
      },
      "outputs": [],
      "source": [
        "log_dir2 = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"-tokenizermodel2\"\n",
        "os.makedirs(log_dir2, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-k51VLEShbc"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDN9MFrBQsrl"
      },
      "outputs": [],
      "source": [
        "from tensorboard.plugins import projector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRo51kRAz2-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53a87fb3-c1a4-46ab-bb5a-8cb47d470537"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "  8/494 [..............................] - ETA: 1:47 - loss: 8.8224 - accuracy: 0.0547"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
        "\n",
        "\n",
        "filepath='/content/drive/MyDrive/Colab Notebooks/best_model_tokenizer_stemming.h5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "num_layers = 2\n",
        "lstm_units_0 = 128\n",
        "recurrent_dropout_0 = 0.2\n",
        "lstm_units_1 = 128\n",
        "recurrent_dropout_1 = 0.1\n",
        "dropout_rate = 0.1\n",
        "learning_rate = 0.005977728042983696\n",
        "\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(vocab_size, 100, input_length=4))\n",
        "model_lstm.add(LSTM(lstm_units_0, return_sequences=True, recurrent_dropout=recurrent_dropout_0))\n",
        "model_lstm.add(LSTM(lstm_units_1, recurrent_dropout=recurrent_dropout_1))\n",
        "model_lstm.add(Dropout(dropout_rate))\n",
        "model_lstm.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir2, histogram_freq=1,\n",
        "    write_graph=True,\n",
        "    write_images=True,\n",
        "    update_freq='epoch',\n",
        ")\n",
        "\n",
        "# fit the model\n",
        "model_lstm.fit(X_train,y_train, epochs=5, batch_size=128, validation_split=0.2, callbacks=[tensorboard_callback,checkpoint])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTgt5WAGZWR2"
      },
      "outputs": [],
      "source": [
        "embedweights=model_lstm.layers[0].get_weights()[0]\n",
        "embedweights.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMlqb12pZdpi"
      },
      "outputs": [],
      "source": [
        "#the following code is adapted from https://www.tensorflow.org/text/guide/word_embeddings#retrieve_the_trained_word_embeddings_and_save_them_to_disk\n",
        "import io\n",
        "\n",
        "out_v = io.open(\"/content/drive/MyDrive/Colab Notebooks/embedding_vectors.tsv\", \"w\", encoding=\"utf-8\")\n",
        "out_m = io.open(\"/content/drive/MyDrive/Colab Notebooks/embedding_metadata.tsv\", \"w\", encoding=\"utf-8\")\n",
        "\n",
        "# Add embedding vectors and words to file\n",
        "for num, word in enumerate(unique_words):\n",
        "  if num == 0:\n",
        "     continue # skip OOV\n",
        "  vec = embedweights[num]\n",
        "  out_m.write(word + \"\\n\") # add the words to file\n",
        "  out_v.write(\"\\t\".join([str(x) for x in vec]) + \"\\n\") # add the corresponding word vector to file\n",
        "out_v.close()\n",
        "out_m.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-HEoSwjEJwv"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir {log_dir2}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyNfqnEbklCgKFfPs8WJ4l+1"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}